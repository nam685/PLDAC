{\rtf1\ansi\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\fi-504\li504\sl240\slmult1\tx504\f0\fs22\lang9 [1]\tab Nawel Astaouti, Thomas Gerald, Maya Touzari, Jian-Yun Nie, and Laure Soulier. MLIA-LIP6@TREC-CAST2021: Feature augmentation for query recontextualization and passage ranking. 7.\par
[2]\tab Nawel Astouati, Thomas Gerald, Maya Touzari, Jian-Yun Nie, and Laure Soulier. Feature augmentation for query recontextualization and passage ranking. 12.\par
[3]\tab Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. \i arXiv:1611.09268 [cs]\i0  (October 2018). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/1611.09268 }}{\fldrslt{http://arxiv.org/abs/1611.09268\ul0\cf0}}}}\f0\fs22\par
[4]\tab Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. CAsT 2020: The Conversational Assistance Track Overview. (2020), 10.\par
[5]\tab Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. TREC CAsT 2019: The Conversational Assistance Track Overview. \i arXiv:2003.13624 [cs]\i0  (March 2020). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2003.13624 }}{\fldrslt{http://arxiv.org/abs/2003.13624\ul0\cf0}}}}\f0\fs22\par
[6]\tab Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. \i arXiv:1810.04805 [cs]\i0  (May 2019). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/1810.04805 }}{\fldrslt{http://arxiv.org/abs/1810.04805\ul0\cf0}}}}\f0\fs22\par
[7]\tab Ahmed Elgohary, Denis Peskov, and Jordan Boyd-Graber. 2019. Can You Unpack That? Learning to Rewrite Questions-in-Context. In \i Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\i0 , Association for Computational Linguistics, Hong Kong, China, 5917\f1\endash\f0 5923. DOI:{{\field{\*\fldinst{HYPERLINK https://doi.org/10.18653/v1/D19-1605 }}{\fldrslt{https://doi.org/10.18653/v1/D19-1605\ul0\cf0}}}}\f0\fs22\par
[8]\tab Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick Craswell. 2022. Neural Approaches to Conversational Information Retrieval. \i arXiv:2201.05176 [cs]\i0  (January 2022). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2201.05176 }}{\fldrslt{http://arxiv.org/abs/2201.05176\ul0\cf0}}}}\f0\fs22\par
[9]\tab Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. \i arXiv:2004.12832 [cs]\i0  (June 2020). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2004.12832 }}{\fldrslt{http://arxiv.org/abs/2004.12832\ul0\cf0}}}}\f0\fs22\par
[10]\tab Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Representations for Ranking using Tightly-Coupled Teachers. \i arXiv:2010.11386 [cs]\i0  (October 2020). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2010.11386 }}{\fldrslt{http://arxiv.org/abs/2010.11386\ul0\cf0}}}}\f0\fs22\par
[11]\tab Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021. In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval. In \i Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)\i0 , Association for Computational Linguistics, Online, 163\f1\endash\f0 173. DOI:{{\field{\*\fldinst{HYPERLINK https://doi.org/10.18653/v1/2021.repl4nlp-1.17 }}{\fldrslt{https://doi.org/10.18653/v1/2021.repl4nlp-1.17\ul0\cf0}}}}\f0\fs22\par
[12]\tab Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021. Contextualized Query Embeddings for Conversational Search. \i arXiv:2104.08707 [cs]\i0  (November 2021). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2104.08707 }}{\fldrslt{http://arxiv.org/abs/2104.08707\ul0\cf0}}}}\f0\fs22\par
[13]\tab Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. TREC 2020 Notebook: CAsT Track. 6.\par
[14]\tab Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. 2021. Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting. \i arXiv:2005.02230 [cs]\i0  (March 2021). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2005.02230 }}{\fldrslt{http://arxiv.org/abs/2005.02230\ul0\cf0}}}}\f0\fs22\par
[15]\tab Gia-Hung Nguyen. Mod\'e8les neuronaux pour la recherche d\rquote information: approches dirig\'e9es par les ressources s\'e9mantiques. 212.\par
[16]\tab Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model. In \i Findings of the Association for Computational Linguistics: EMNLP 2020\i0 , Association for Computational Linguistics, Online, 708\f1\endash\f0 718. DOI:{{\field{\*\fldinst{HYPERLINK https://doi.org/10.18653/v1/2020.findings-emnlp.63 }}{\fldrslt{https://doi.org/10.18653/v1/2020.findings-emnlp.63\ul0\cf0}}}}\f0\fs22\par
[17]\tab Rodrigo Nogueira and Jimmy Lin. From doc2query to docTTTTTquery. 3.\par
[18]\tab Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document Expansion by Query Prediction. \i arXiv:1904.08375 [cs]\i0  (September 2019). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/1904.08375 }}{\fldrslt{http://arxiv.org/abs/1904.08375\ul0\cf0}}}}\f0\fs22\par
[19]\tab Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\'e4schel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. \i arXiv:2009.02252 [cs]\i0  (May 2021). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2009.02252 }}{\fldrslt{http://arxiv.org/abs/2009.02252\ul0\cf0}}}}\f0\fs22\par
[20]\tab Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models. \i arXiv:2101.05667 [cs]\i0  (January 2021). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/2101.05667 }}{\fldrslt{http://arxiv.org/abs/2101.05667\ul0\cf0}}}}\f0\fs22\par
[21]\tab Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. \i arXiv:1910.10683 [cs, stat]\i0  (July 2020). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/1910.10683 }}{\fldrslt{http://arxiv.org/abs/1910.10683\ul0\cf0}}}}\f0\fs22\par
[22]\tab Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. \i arXiv:1706.03762 [cs]\i0  (December 2017). Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://arxiv.org/abs/1706.03762 }}{\fldrslt{http://arxiv.org/abs/1706.03762\ul0\cf0}}}}\f0\fs22\par
[23]\tab Laure Soulier. D\'e9finition et \'e9valuation de mod\'e8les de recherche d\rquote information collaborative bas\'e9s sur les comp\'e9tences de domaine et les r\'f4les des utilisateurs. Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK http://thesesups.ups-tlse.fr/2735/1/2014TOU30190.pdf }}{\fldrslt{http://thesesups.ups-tlse.fr/2735/1/2014TOU30190.pdf\ul0\cf0}}}}\f0\fs22\par
[24]\tab TREC Conversational Assistance Track (CAsT). \i The TREC Conversational Assistance Track (CAsT)\i0 . Retrieved February 20, 2022 from {{\field{\*\fldinst{HYPERLINK https://www.treccast.ai/ }}{\fldrslt{https://www.treccast.ai/\ul0\cf0}}}}\f0\fs22\par
}
 