{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finetune T5 CQR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOggFNqy+nSSPEXthNKSaDr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdQxAbZ-jYnJ","executionInfo":{"status":"ok","timestamp":1645888262665,"user_tz":-60,"elapsed":21832,"user":{"displayName":"Nam Lê Hải","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04547901085605070272"}},"outputId":"e6b07a35-6ad1-4fa3-da1d-dd4b690c5519"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/PLDAC\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/PLDAC/"]},{"cell_type":"code","source":["!pip install transformers -q\n","!pip install wandb -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TsHzBJfQjpHC","executionInfo":{"status":"ok","timestamp":1645888283831,"user_tz":-60,"elapsed":18022,"user":{"displayName":"Nam Lê Hải","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04547901085605070272"}},"outputId":"13ab25a4-a4a3-4a3d-b0c7-37b196914b78"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 31.1 MB/s \n","\u001b[K     |████████████████████████████████| 6.8 MB 34.7 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 40.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 4.2 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 48.1 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 38.7 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# WandB – Import the wandb library\n","import wandb"],"metadata":{"id":"aiqh9uOtkP_S","executionInfo":{"status":"ok","timestamp":1645888291503,"user_tz":-60,"elapsed":7688,"user":{"displayName":"Nam Lê Hải","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04547901085605070272"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Df0gL6pTlNWC","executionInfo":{"status":"ok","timestamp":1645888291505,"user_tz":-60,"elapsed":36,"user":{"displayName":"Nam Lê Hải","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04547901085605070272"}},"outputId":"bf7dec85-923c-4722-bdaa-501c16281956"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Feb 26 15:11:34 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   70C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"metadata":{"id":"dkavBkO4let0","executionInfo":{"status":"ok","timestamp":1645888292113,"user_tz":-60,"elapsed":622,"user":{"displayName":"Nam Lê Hải","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04547901085605070272"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2daoWX6lhxD","executionInfo":{"status":"ok","timestamp":1645888395694,"user_tz":-60,"elapsed":7673,"user":{"displayName":"Nam Lê Hải","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04547901085605070272"}},"outputId":"5c543448-3d4a-43c5-beda-a6cd9d4c4563"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"metadata":{"id":"5O8BOa7Gl7bQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n","# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n","        loss = outputs[0]\n","        \n","        if _%10 == 0:\n","            wandb.log({\"Training Loss\": loss.item()})\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"cMJ9wIORxYR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=150, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"metadata":{"id":"1KnUCn2hxYUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_tutorials_summarization\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training  \n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1 \n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    config.MAX_LEN = 512\n","    config.SUMMARY_LEN = 150 \n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","    \n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only. \n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n","    df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n","    df = df[['text','ctext']]\n","    df.ctext = 'summarize: ' + df.ctext\n","    print(df.head())\n","\n","    \n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","    \n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        final_df.to_csv('./models/predictions.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"9MRxBRnIxYXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dcAS8y2UxeVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Zl3rMONYxeXz"},"execution_count":null,"outputs":[]}]}